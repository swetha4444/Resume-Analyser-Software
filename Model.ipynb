{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "1ee4784502f22c791f4aad474eada1f04ffa3b3f5be6de76313108a032fb3e03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Embedding, LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import metrics, regularizers\n",
    "from keras.preprocessing import sequence\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load cleaned dataset\n",
    "data = pd.read_csv('Cleaned_JobDescs.csv', header = 0, names = ['Query', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split the dataset to Training and Test subsets (90/10)\n",
    "train, test = train_test_split(data, test_size = 0.1, random_state = 17) #random_state = None\n",
    "\n",
    "train_descs = train['Description']\n",
    "train_labels = train['Query']\n",
    " \n",
    "test_descs = test['Description']\n",
    "test_labels = test['Query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "vocab_size = 1000\n",
    "\n",
    "sequences_length = 1200\n",
    "\n",
    "embedding_dimensionality = 64 #possibly low??\n",
    "max_features = 2000 #equal to vocab_size\n",
    "\n",
    "num_labels = len(train_labels.unique())\n",
    "batch_size = 32\n",
    "nb_epoch = 20\n",
    "\n",
    "nof_filters = 200\n",
    "kernel_size = 16\n",
    "\n",
    "hidden_dims = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert Texts to Numeric Vectors for Input\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(train_descs)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(train_descs)\n",
    "x_test = tokenizer.texts_to_sequences(test_descs)\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = sequences_length, padding = 'post')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = sequences_length, padding = 'post')\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 1200, 64)          128000    \n_________________________________________________________________\nconv1d (Conv1D)              (None, 1185, 200)         205000    \n_________________________________________________________________\nglobal_max_pooling1d (Global (None, 200)               0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               102912    \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\nactivation (Activation)      (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 25)                12825     \n_________________________________________________________________\nactivation_1 (Activation)    (None, 25)                0         \n=================================================================\nTotal params: 448,737\nTrainable params: 448,737\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dimensionality, input_length = 1200))\n",
    "#model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Conv1D(nof_filters, kernel_size, padding='valid', activation='relu', strides = 1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', #'sgd', 'adam', 'RMSprop', 'Adagrad'\n",
    "                   metrics = [metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "225/225 [==============================] - 143s 584ms/step - loss: 3.1601 - categorical_accuracy: 0.0637 - val_loss: 2.0208 - val_categorical_accuracy: 0.3822\n",
      "Epoch 2/20\n",
      "225/225 [==============================] - 130s 580ms/step - loss: 1.8429 - categorical_accuracy: 0.4253 - val_loss: 1.4427 - val_categorical_accuracy: 0.5489\n",
      "Epoch 3/20\n",
      "225/225 [==============================] - 131s 584ms/step - loss: 1.1501 - categorical_accuracy: 0.6382 - val_loss: 1.2295 - val_categorical_accuracy: 0.6189\n",
      "Epoch 4/20\n",
      "225/225 [==============================] - 134s 597ms/step - loss: 0.7294 - categorical_accuracy: 0.7734 - val_loss: 1.1599 - val_categorical_accuracy: 0.6489\n",
      "Epoch 5/20\n",
      "225/225 [==============================] - 128s 570ms/step - loss: 0.4768 - categorical_accuracy: 0.8559 - val_loss: 1.1792 - val_categorical_accuracy: 0.6561\n",
      "Epoch 6/20\n",
      "225/225 [==============================] - 113s 501ms/step - loss: 0.2854 - categorical_accuracy: 0.9224 - val_loss: 1.2179 - val_categorical_accuracy: 0.6561\n",
      "Epoch 7/20\n",
      "225/225 [==============================] - 108s 481ms/step - loss: 0.1688 - categorical_accuracy: 0.9650 - val_loss: 1.3230 - val_categorical_accuracy: 0.6506\n",
      "Epoch 8/20\n",
      "225/225 [==============================] - 135s 600ms/step - loss: 0.1200 - categorical_accuracy: 0.9739 - val_loss: 1.3743 - val_categorical_accuracy: 0.6522\n",
      "Epoch 9/20\n",
      "225/225 [==============================] - 118s 524ms/step - loss: 0.0979 - categorical_accuracy: 0.9821 - val_loss: 1.3984 - val_categorical_accuracy: 0.6572\n",
      "Epoch 10/20\n",
      "225/225 [==============================] - 115s 513ms/step - loss: 0.0947 - categorical_accuracy: 0.9828 - val_loss: 1.4185 - val_categorical_accuracy: 0.6600\n",
      "Epoch 11/20\n",
      "225/225 [==============================] - 131s 581ms/step - loss: 0.1001 - categorical_accuracy: 0.9827 - val_loss: 1.4867 - val_categorical_accuracy: 0.6467\n",
      "Epoch 12/20\n",
      "225/225 [==============================] - 167s 741ms/step - loss: 0.0840 - categorical_accuracy: 0.9847 - val_loss: 1.4390 - val_categorical_accuracy: 0.6617\n",
      "Epoch 13/20\n",
      "225/225 [==============================] - 196s 873ms/step - loss: 0.0790 - categorical_accuracy: 0.9840 - val_loss: 1.5052 - val_categorical_accuracy: 0.6611\n",
      "Epoch 14/20\n",
      "225/225 [==============================] - 197s 874ms/step - loss: 0.0788 - categorical_accuracy: 0.9880 - val_loss: 1.5285 - val_categorical_accuracy: 0.6628\n",
      "Epoch 15/20\n",
      "225/225 [==============================] - 142s 629ms/step - loss: 0.0716 - categorical_accuracy: 0.9863 - val_loss: 1.5329 - val_categorical_accuracy: 0.6672\n",
      "Epoch 16/20\n",
      "225/225 [==============================] - 182s 812ms/step - loss: 0.0637 - categorical_accuracy: 0.9874 - val_loss: 1.5239 - val_categorical_accuracy: 0.6506\n",
      "Epoch 17/20\n",
      "225/225 [==============================] - 155s 690ms/step - loss: 0.0531 - categorical_accuracy: 0.9870 - val_loss: 1.5991 - val_categorical_accuracy: 0.6494\n",
      "Epoch 18/20\n",
      "225/225 [==============================] - 195s 866ms/step - loss: 0.0495 - categorical_accuracy: 0.9868 - val_loss: 1.5448 - val_categorical_accuracy: 0.6583\n",
      "Epoch 19/20\n",
      "225/225 [==============================] - 159s 706ms/step - loss: 0.0511 - categorical_accuracy: 0.9879 - val_loss: 1.6246 - val_categorical_accuracy: 0.6717\n",
      "Epoch 20/20\n",
      "225/225 [==============================] - 138s 614ms/step - loss: 0.0438 - categorical_accuracy: 0.9890 - val_loss: 1.6222 - val_categorical_accuracy: 0.6567\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = nb_epoch,\n",
    "                    verbose = True,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "user_text = \"Complete detailed programming and development tasks for front end public and internal websites as well as challenging back-end server code.\"\n",
    "\n",
    "# Encode the text\n",
    "encoded_docs = [one_hot(user_text, 500)]\n",
    "# pad documents to a max length\n",
    "padded_text = pad_sequences(encoded_docs, maxlen=500, padding='post')\n",
    "# Prediction based on model\n",
    "prediction = model.predict(padded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the prediction\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(test_labels)\n",
    "result = encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Business Intelligence Analyst']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}